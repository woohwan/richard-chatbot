{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BedRcok Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boto3 session for interacting with AWS\n",
    "import boto3\n",
    "session = boto3.Session(region_name='us-east-1')\n",
    "# create bedrock client\n",
    "br_client = session.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluade-v2 LLM Model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM Model using claude-v2 model and langchain\n",
    "from langchain.chat_models import BedrockChat\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# https://python.langchain.com/docs/integrations/chat/bedrock\n",
    "chat_model = BedrockChat(\n",
    "  model_id=\"anthropic.claude-v2\",\n",
    "  client=br_client,\n",
    "  model_kwargs={\n",
    "    \"max_tokens_to_sample\": 512,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "  },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenSearch RAG\n",
    "임베딩은 embedding opensearch 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenSearch 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'DESKTOP-WHPARK', 'cluster_name': 'opensearch', 'cluster_uuid': 'Fu1wEBfcT_OjptkNYtB1WA', 'version': {'distribution': 'opensearch', 'number': '2.11.1', 'build_type': 'deb', 'build_hash': '6b1986e964d440be9137eba1413015c31c5a7752', 'build_date': '2023-11-29T21:43:44.221253956Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "\n",
    "#  opensearch info\n",
    "host = 'localhost'\n",
    "port = 9200\n",
    "http_auth = ('admin', 'admin')\n",
    "opensearch_endpoint = f'https://{host}:{port}'\n",
    "\n",
    "# certs info: https://www.notion.so/Certificate-Password-8087893ff1b24dd0b71e66c9916fa55a\n",
    "ca_cert = 'certs/root-ca.pem'\n",
    "client_cert = 'certs/admin.pem'\n",
    "client_key = 'certs/admin-key.pem'\n",
    "\n",
    "\n",
    "# create client\n",
    "os_client = OpenSearch(\n",
    "    hosts=[opensearch_endpoint],\n",
    "    http_auth=http_auth,\n",
    "    ca_certs=ca_cert,\n",
    "    client_cert=client_cert,\n",
    "    client_key=client_key,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "# verify connection\n",
    "print(os_client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenSearch VectorStore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a embeding model\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "llm_emb = BedrockEmbeddings(\n",
    "    model_id='amazon.titan-embed-text-v1',\n",
    "    client=br_client,\n",
    "    region_name='us-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "index_name = \"constitution-with-nori\"\n",
    "\n",
    "vector_db = OpenSearchVectorSearch(\n",
    "    opensearch_url=opensearch_endpoint,\n",
    "    index_name=index_name,\n",
    "    embedding_function=llm_emb,\n",
    "    http_auth=http_auth,\n",
    "    ca_certs=ca_cert,\n",
    "    client_cert=client_cert,\n",
    "    client_key=client_key,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query=\"민주공화국\"\n",
    "# docs = vector_db.similarity_search(query=query, k=3)\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reriever\n",
    "retriever = vector_db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "# https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "# define a prompt template\n",
    "template= \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Qeustion: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "  return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Create conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "chain = (\n",
    "        {\"context\": itemgetter(\"question\")| retriever | format_docs, \n",
    "         \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | chat_model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "# chain.invoke(\"헌법 1조 2항은?\")\n",
    "\n",
    "def stream_response(input, history):\n",
    "  if input is not None:\n",
    "    partial_message = \"\"\n",
    "    print(input)\n",
    "    for response in chain.stream({\"question\": input}):\n",
    "      partial_message += response\n",
    "      yield partial_message\n",
    "\n",
    "# https://www.gradio.app/docs/chatinterface\n",
    "gr.ChatInterface(stream_response, title=\"Chat with Claude\").queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
